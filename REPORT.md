# DP(Differential Privacy)로 MIA(Membership Inference Attack) 방어하기 실습

**학번:** 202540487  
**이름:** 송영빈  
**날짜:** 2025년 12월 3일

---

## 1. 실험 목표

Differential Privacy(DP)를 적용하여 Membership Inference Attack(MIA)으로부터 모델을 방어하고, Privacy-Utility Trade-off를 분석한다.

---

## 2. 실험 환경

| 항목 | 설정 |
|------|------|
| **데이터셋** | IMDb 영화 리뷰 (25,000 train / 25,000 test) |
| **모델** | DistilBERT (distilbert-base-uncased) |
| **DP 라이브러리** | Opacus |
| **하드웨어** | CUDA GPU |
| **프레임워크** | PyTorch, Hugging Face Transformers |

### DP-SGD 하이퍼파라미터

| 파라미터 | 값 | 설명 |
|---------|-----|------|
| `noise_multiplier` | 0.5 | 그래디언트에 추가되는 노이즈 비율 |
| `max_grad_norm` | 1.0 | 그래디언트 클리핑 임계값 |
| `epochs` | 5 | DP 학습 에포크 수 |
| `batch_size` | 16 | 배치 크기 |
| `learning_rate` | 1e-3 | Classifier 학습률 |
| `delta (δ)` | 1e-05 | 프라이버시 파라미터 |

---

## 3. 데이터셋 분할

```
┌─────────────────────────────────────────────────────────────┐
│                    IMDb Train (25,000)                       │
├─────────────────────────────┬───────────────────────────────┤
│      A1 (12,500)            │         A2 (12,500)           │
│   타겟 모델 학습용           │      MIA 멤버 평가용           │
└─────────────────────────────┴───────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                    IMDb Test (25,000)                        │
├──────────────┬──────────────┬───────────────────────────────┤
│  B1 (6,250)  │  B2 (6,250)  │        B3 (12,500)            │
│ 새도우 학습   │ 새도우 비멤버 │     MIA 비멤버 평가용          │
└──────────────┴──────────────┴───────────────────────────────┘
```

---

## 4. MIA 공격 방법

### 4.1 Shadow Model Attack (Shokri et al., 2017)

1. **새도우 모델 학습**: 타겟 모델과 유사한 구조의 모델을 B1 데이터로 학습
2. **특징 추출**: 새도우 모델의 출력 confidence를 특징으로 사용
3. **공격 모델 학습**: MLP로 멤버/비멤버 분류기 학습
4. **공격 수행**: 타겟 모델의 출력으로 멤버십 추론

### 4.2 Simple Confidence Attack

- 모델의 예측 confidence가 높으면 멤버로 추론
- 학습 데이터에 대해 모델이 더 확신있게 예측한다는 가정

---

## 5. 실험 결과

### 5.1 분류 성능 비교

| 모델 | A2 (Holdout) 정확도 | B3 (Nonmember) 정확도 |
|------|--------------------|-----------------------|
| **비보호 타겟** | 86.74% | 86.48% |
| **DP 보호 타겟** | 77.57% | 76.30% |
| **차이** | -9.17%p | -10.18%p |

### 5.2 MIA 공격 결과

| 공격 방법 | 비보호 모델 AUC | DP 모델 AUC |
|----------|----------------|-------------|
| **Shadow + MLP** | 0.5025 | 0.5031 |
| **Simple Confidence** | 0.5034 | 0.5065 |

> **AUC = 0.5**는 랜덤 추측 수준으로, 공격이 효과적이지 않음을 의미

### 5.3 Privacy Budget

| 파라미터 | 값 |
|---------|-----|
| **ε (epsilon)** | 4.80 |
| **δ (delta)** | 1e-05 |

- ε이 작을수록 강한 프라이버시 보장
- 일반적으로 ε < 10이면 실용적인 프라이버시 수준

---

## 6. Privacy-Utility Trade-off 분석

```
┌────────────────────────────────────────────────────────────┐
│                  Privacy-Utility Trade-off                  │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  정확도 (%)                                                 │
│  100 ┤                                                     │
│   90 ┤  ●─────── 비보호 모델 (86.74%)                       │
│   80 ┤          ╲                                          │
│   70 ┤           ●─── DP 모델 (77.57%, ε=4.80)             │
│   60 ┤                                                     │
│   50 ┤                                                     │
│      └──────────────────────────────────────────────────   │
│         0        2        4        6        8       10     │
│                        epsilon (ε)                         │
└────────────────────────────────────────────────────────────┘
```

### Trade-off 요약

| 항목 | 비보호 모델 | DP 모델 | 변화 |
|------|-----------|---------|------|
| 분류 정확도 | 86.74% | 77.57% | **-9.17%p** |
| 프라이버시 보장 | 없음 (ε=∞) | ε=4.80 | **강한 보장** |
| MIA 방어 | AUC ≈ 0.50 | AUC ≈ 0.50 | 유지 |

---

## 7. 결론

### 7.1 주요 발견

1. **DP-SGD 효과**: Differential Privacy를 적용하여 ε=4.80의 프라이버시 보장을 달성
2. **정확도 손실**: 약 9.17%p의 정확도 하락으로 합리적인 trade-off
3. **MIA 방어**: 원래 모델도 MIA에 취약하지 않았으나, DP 적용으로 수학적 프라이버시 보장 획득

### 7.2 MIA AUC가 0.5인 이유

- IMDb 데이터셋이 충분히 크고 (25,000개) 다양함
- DistilBERT가 잘 일반화되어 개별 데이터를 과적합(memorization)하지 않음
- 이는 모델이 프라이버시 친화적으로 동작함을 의미

### 7.3 실용적 시사점

> "DP-SGD를 적용하면 **약 10%의 정확도 손실**로 **강력한 수학적 프라이버시 보장 (ε=4.80)**을 얻을 수 있다. 이는 민감한 데이터를 다루는 실제 서비스에서 합리적인 trade-off이다."

---

## 8. 참고 문헌

1. Shokri, R., et al. (2017). "Membership Inference Attacks Against Machine Learning Models." IEEE S&P.
2. Abadi, M., et al. (2016). "Deep Learning with Differential Privacy." CCS.
3. Opacus Documentation: https://opacus.ai/

---

## 부록: 코드 구조

```
dp_mia_lab/
├── dp_mia_defense.py    # 메인 실험 코드
├── requirements.txt     # 의존성 패키지
├── run.sh              # 실행 스크립트
└── README.md           # 프로젝트 설명
```

### 실행 방법

```bash
# 의존성 설치
pip install -r requirements.txt

# 실험 실행
python dp_mia_defense.py
```

